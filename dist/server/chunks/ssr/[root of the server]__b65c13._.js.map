{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 7, "column": 0}, "map": {"version":3,"sources":["file:///Users/aryan/projects/personalwebsite/src/data/blogs.ts"],"sourcesContent":["export type BlogPost = {\n  id: string;\n  title: string;\n  slug: string;\n  date: string;\n  summary: string;\n  content: string;\n  coverImage?: string;\n  isProjectBlog: boolean;\n  projectId?: number;\n  tags: string[];\n};\n\nexport const blogPosts: BlogPost[] = [\n  {\n    id: \"1\",\n    title: \"Hyperloo: Building a Knowledge Graph for University Courses\",\n    slug: \"hyperloo-project-blog\",\n    date: \"2025-04-23\",\n    summary: \"How I built an algorithmically generated knowledge graph for UWaterloo programs and courses\",\n    content: `\nHyperloo is a knowledge graph that maps out all topics, courses, and degrees at the University of Waterloo. Traditional degree structures tend to be abstract, making it difficult to visualize the interconnected nature of knowledge. I wanted to change that. What if you could represent an entire degree visually - showing how concepts interlink and build on one another? That question led me deep into knowledge graphs, which I found incredibly powerful as a tool for structuring and exploring complex domains.\n\n## Why Knowledge Graphs?\n\nConcepts are inherently nested objects. Take nuclear physics - understanding the entire field seems daunting. But break it down, and it's really just a collection of interconnected subtopics. Each of those subtopics can be further divided, creating a layered structure that makes even the most complex subjects feel approachable. Knowledge graphs embody this philosophy: no topic is truly out of reach if broken down correctly. Seeing how subjects connect makes learning more intuitive, empowering students to explore topics they once thought were beyond them.\n\nOne of my key inspirations was the Socratica graph matching from the 2023 Socratica Symposium. As graph tooling continues to improve, I believe we'll see knowledge graphs become a standard way to represent and navigate information.\n\n## Building Hyperloo\n\nCreating Hyperloo was a long and technically challenging process. The first step was data collection: scraping every Waterloo syllabus to form a base knowledge corpus. We used Selenium to automate the scraping, parsing programs, courses, and subtopics iteratively. This required multiple browser automation scripts to extract structured data from unstructured web pages. The process was labor-intensive, but necessary.\n\nOnce we had a structured corpus, the next challenge was transforming raw syllabus text into meaningful graph data. We trained a custom NLP model using SpaCy to extract key information. The NLP pipeline was relatively simple - a classification model trained with labeled examples to recognize important syllabus components. After multiple iterations, we achieved ~92% accuracy, which was sufficient for our needs. The model's purpose wasn't perfect precision but rather rough approximation to filter syllabus content into useful knowledge nodes.\n\nWith the extracted information, we structured the data into a nested JSON format that could be easily visualized as a graph. To generate these structured JSON objects, we used OpenAI's Batch API, processing large volumes of text and distilling them into a structured hierarchy. The result was a massive JSON-L file representing Waterloo's academic knowledge as a network of interconnected topics.\n\nFinally, we built the front end using React, leveraging the GraphForce component to render the knowledge graph. While some customization was required to optimize the visualization, the hardest part of the project was the data transformation itself - getting from raw syllabi to structured knowledge nodes.\n\n## Impact and Utility\n\nHyperloo has already gained traction. After sharing it on LinkedIn and Twitter, it received over 250 likes on LinkedIn and 100+ on Twitter. More importantly, it was bookmarked 17 times on Twitter - an indicator that people actually intend to use it as a reference.\n\nFor Waterloo students, the utility is clear. Hyperloo provides a structured, visual way to explore degree programs, understand prerequisite relationships, and dive into any topic of interest. Even if only a few dozen students actively use it, that's a meaningful outcome for me.\n\nBut the implications go beyond Waterloo. With Hyperloo, anyone, anywhere in the world, can effectively trace the structure of a Waterloo degree and use it as a self-learning roadmap. Even though it's not a complete curriculum, the ability to map out an entire field and navigate it freely is incredibly powerful. In theory, a student in a third world nation or any remote region could use Hyperloo, coupled with Perplexity AI and other online resources, to pursue an entire degree's worth of knowledge for free.\n\n## What's Next?\n\nBefore starting any project, I ask myself: if I were to disappear tomorrow, what would I leave behind? Hyperloo is one of those projects that feels genuinely useful - not just to me, but to the broader world. If it grows, it could be a foundational resource for structured, open-access education. That's the kind of impact worth building for.\n\nCheck out the production version of Hyperloo [here](https://tumph.github.io/hyperlooprod/).\n\n\n    `,\n    coverImage: \"/hyperloo.png\",\n    isProjectBlog: true,\n    projectId: 1,\n    tags: [\"NLP\", \"Web Scraping\", \"Knowledge Graph\", \"Next.js\"]\n  },\n  {\n    id: \"2\",\n    title: \"Doledesk: Automating Substitute Teacher Scheduling\",\n    slug: \"doledesk-project-blog\",\n    date: \"2023-03-15\",\n    summary: \"How I built a system to automate substitute teacher scheduling\",\n    content: `\nIn 2023, I started looking into inefficiencies in school operations - areas where automation could replace tedious, repetitive processes. Automation was becoming increasingly practical, and I wanted to build something with real impact. That led me to a conversation with my high school's vice principal. I asked a simple question: “What are the most time-consuming tasks you deal with daily?”\n\nThe answer was clear: substitute teacher scheduling. Every morning, administrators scrambled to check which teachers were absent, cross-referencing a massive spreadsheet to manually assign substitutes. It was an inefficient, error-prone process - one that was ripe for automation. That insight led me to build Doledesk, a system designed to fully automate substitute teacher scheduling using a rules-based algorithm.\n\n## The Complexity of Scheduling\n\nAt first, scheduling substitutes seemed like a straightforward problem - identify absent teachers and match them with available subs. But the deeper I went, the more complexity I uncovered. Several constraints made this a non-trivial problem:\n\n- Legal Compliance: Many school districts have strict labor laws. For example, a teacher can't work more than three consecutive periods without a break.\n\n- Subject Matching: Not all substitutes can teach every subject. A math teacher shouldn't be assigned to an English class.\n\n- Multi-Layered Dependencies: If one substitute isn't available, the entire schedule may need to be reshuffled dynamically.\n\n- Data Privacy Restrictions: Storing teacher data required careful adherence to K-12 privacy regulations, meaning traditional database solutions weren't viable.\n\nThese constraints required a robust, flexible system capable of handling real-world edge cases while optimizing for efficiency.\n\n## Building Doledesk\n\nThe core of Doledesk was a backend system built with Java and JavaScript, designed to process teacher absences and dynamically assign substitutes. Here's how it worked:\n\n- Data Input Pipeline: Each morning, a fresh list of absent teachers was fed into the system. Since storing persistent data wasn't an option due to privacy regulations, all scheduling had to happen in real-time.\n\n- Algorithmic Matching: The backend used a constraint-satisfaction algorithm to assign substitutes based on availability, subject expertise, and legal guidelines. If an optimal match wasn't found, the algorithm recursively adjusted placements.\n\n- Automated Notifications: Once schedules were finalized, the system sent out automated email notifications to substitutes and teachers.\n\n- Failsafe Mechanisms: If any substitute declined their assignment, the system reran the matching algorithm to fill gaps dynamically.\n\nFor the UI, I opted for Bubble, a no-code editor, to accelerate frontend development. This allowed me to quickly iterate on the UI and get a working product.\n\n## Deployment and Impact\n\nOnce Doledesk was live, my school used it for a month. The result? Administrators who previously spent hours manually assigning substitutes were now completing the process in minutes. What used to be a chaotic, last-minute scramble was now a structured, automated workflow.\n\nWhile this project started as an experiment, it quickly became something more - proof that automation can significantly reduce administrative overhead in education. Doledesk wasn't just about saving time; it was about ensuring that students always had the right teachers in place, improving the overall classroom experience.\n\n## What's Next?\n\nDoledesk validated an important idea: many outdated, manual processes in education can be automated with the right approach. Looking forward, I see opportunities to expand this concept beyond substitute scheduling - perhaps into broader school operations or even district-wide automation tools.\n    `,\n    coverImage: \"/doledesk.png\",\n    isProjectBlog: true,\n    projectId: 2,\n    tags: [\"Java\", \"JavaScript\", \"Bubble\"]\n  },\n  {\n    id: \"3\",\n    title: \"Pare: Summarizing Resumes on Any ATS\",\n    slug: \"pare-project-blog\",\n    date: \"2024-09-3\",\n    summary: \"How I built a browser extension to summarize resumes on any ATS\",\n    content: `\n## The Hiring Challenge\n\nIn 2024, I embarked on a project to tackle inefficiencies in the recruiting industry, a journey inspired by my experience with the Waterloo Co-op program. My first co-op was at an AI company called ada, and while the opportunity was exciting, the hiring process itself was frustratingly arduous. Securing the position required significant effort, and the lack of feedback or even rejection emails while applying to other jobs left me feeling like my resume was getting lost in the void.\n\nAt first, I blamed recruiters, assuming they weren't reviewing resumes properly. However, after speaking with several recruiters, I realized the problem wasn't them - it was the sheer volume of applications they had to process daily. Some recruiters I spoke to had thousands of resumes to sift through, often spending only 5-10 seconds per resume. Given these constraints, it became clear that recruiters simply didn't have the bandwidth to provide personalized responses to every applicant.\n\nThis insight led me to ask: Could technology streamline this process and help recruiters make better decisions, faster? That's when I built Pare, a resume summarization tool designed to make the recruiting process more efficient.\n\n## Introducing Pare: A Resume Summarizer for ATS Platforms\n\nPare is a browser extension that integrates with any Applicant Tracking System (ATS) to help recruiters process resumes faster. For those unfamiliar, an ATS is the software recruiters use to track applicants, manage job postings, and oversee hiring pipelines - it's like CRM software but tailored for recruiting.\n\nSince recruiters only have a few seconds to assess a resume, I saw an opportunity to reduce cognitive load by providing structured, AI-generated summaries. The goal was simple: help recruiters get to the essence of a candidate's experience in a fraction of the time.\n\n## The Technical Approach\n\nBuilding Pare came with significant technical challenges. The primary complexity lay in making the extension work seamlessly across multiple ATS platforms, each with its own data structures and UI implementations. Some ATS platforms rely on React components, others use HTML pop-ups, and some simply display plain text.\n\nHere's how Pare works:\n\n- Resume Extraction: Pare uses PDF.js to extract the text from resumes displayed in ATS platforms.\n\n- AI-Powered Summarization: The extracted resume text is then fed into OpenAI's GPT model with structured prompting to generate a concise, recruiter-friendly summary.\n\n- Dynamic Integration: The extension identifies different ATS architectures and adapts accordingly to display the summarized resume in the appropriate section of the UI.\n\n## Overcoming Edge Cases\n\nA key challenge in developing Pare was ensuring its functionality across different ATS platforms. Some ATS systems store resume data in embedded JavaScript objects, while others use dynamically generated iframes. To address these variations, Pare includes:\n\n- DOM Inspection and Adaptation: The extension dynamically detects how the ATS renders resumes and adapts its extraction process accordingly.\n\n- Asynchronous Handling: Since ATS platforms load data asynchronously, Pare waits for DOM elements to fully render before extracting content, preventing errors from incomplete data.\n\n- Cross-Origin Requests: Some ATS platforms restrict direct access to resume content, requiring workarounds like injecting scripts into the page context to retrieve the necessary data.\n\n## Adoption and Impact\n\nSince launching Pare, several recruiters - many of whom I connected with during the development process - have started using it. A few of my friends running startups with high hiring volumes have also adopted it to streamline their recruiting workflows.\n\nThe extension is available on GitHub, where users can configure it with their own OpenAI API key and install it as an unpacked Chrome extension via Developer Tools. By reducing the time recruiters spend reading resumes, Pare helps them focus on identifying the best candidates rather than drowning in an overwhelming sea of applications.\n\n## What's Next?\n\nPare is just the beginning. There are still many inefficiencies in the hiring process that can be optimized with AI-driven tools. Future iterations of Pare could incorporate:\n\n- Customizable Summarization Styles: Allowing recruiters to tweak summary formats based on industry-specific needs.\n\n- Multi-Resume Comparisons: Automatically highlighting key differentiators between candidates.\n\n- Integration with More ATS Platforms: Expanding native support for widely used recruiting systems.\n\nAt its core, Pare is about making hiring more efficient - both for recruiters and job seekers. By leveraging AI to handle tedious, high-volume tasks, recruiters can spend more time on what truly matters: connecting great candidates with great opportunities.\n    `,\n    coverImage: \"/pare.png\",\n    isProjectBlog: true,\n    projectId: 3,\n    tags: [\"JavaScript\", \"HTML/CSS\", \"Git\", \"PDF.js\"]\n  },\n  {\n    id: \"4\",\n    title: \"SAI Microjet: Optimizing Sulfur Ratios for Geoengineering\",\n    slug: \"sai-microjet-project-blog\",\n    date: \"2022-07-09\",\n    summary: \"Developing a hardware rig and firmware system for a microjet engine to test optimal sulfur ratios for stratospheric aerosol injection.\",\n    content: `\n## A Climate Engineering Experiment\n\nClimate change is one of the most pressing challenges of our time. At its core, it is an energy problem - greenhouse gases trap more solar radiation in the Earth's atmosphere, increasing surface temperatures.\n\nIf you think about climate change from a physics perspective - Earth is essentially a self contained system (all the energy that arrives on Earth from the sun gets dispersed in some way or form - energy cannot be created or destroyed!) and so a thicker atmosphere with higher PPMs of greenhouse gases that trap energy more effectively would lead to more of that energy being dispersed as heat in the atmosphere instead of staying as radiation being reflected back into space. \n\nCurrent efforts to mitigate climate change generally fall into two categories: reducing carbon emissions (by transitioning to renewable energy) and removing carbon already in the atmosphere (through carbon capture technologies). However, this ignores the fact that from a physics perspective, if you want to reduce the energy trapped in a system - you could increase permabilility of the system so less energy is trapped (current methods) - but you could also just prevent that energy from reaching the system in the first place.\n\nGeoengineering strategies aim to reduce the amount of solar radiation reaching Earth. Among the various proposals - such as placing giant mirrors in space or brightening marine clouds - one of the most promising and realistic methods (not as pie in the sky as the other ones) is stratospheric aerosol injection (SAI). This technique involves releasing small aerosol particles, such as sulfur compounds, into the stratosphere to reflect sunlight and cool the planet, similar to how volcanic eruptions impact global temperatures. The reason this is the most realistic is this has actually happened naturally before - A historical example of this effect was the 1991 eruption of Mount Pinatubo, which led to a temporary global cooling of 1-2 degrees Celsius.\n\nEven if you reduce the amount of sunlight reaching Earth by 1-2%, you would completely negate the 2-3 centuries of anthropogenic climate change we have experienced up till today. Isn't that crazy? And the effect to plant fauna would be negligible - would you really be thirsty if your glass of water had 1% less water in it?\n\n## A Practical Approach: Sulfur Injection via Jet Engine Fuel\n\nRather than deploying a specialized fleet of aircraft carrying sulfur dioxide tanks for stratospheric aerosol injection (as current research is doing), my project explored a more efficient approach: integrating sulfur directly into jet fuel. This concept allows aircraft to release sulfur precursors passively as they fly, eliminating the need for dedicated spraying equipment.\n\nMy research, conducted with Andrew Lockley from the University College London, began with a conceptual analysis of this approach. However, to validate the feasibility of sulfur-infused jet fuel, I moved beyond theory into practical experimentation.\n\n## Testing with a Microjet Engine\n\nTo test the concept, I acquired a microjet engine - a scaled-down jet engine that operates on the same physical principles as full-scale aircraft turbines. Instead of experimenting on a multimillion-dollar Rolls-Royce engine, a microjet provided a cost-effective, low-risk way to evaluate fuel modifications.\n\n## Experiment Design\n\nThe experiment involved three key components:\n\n- Engine Modification & Sensor Integration:\n\nThe microjet engine had built-in safety mechanisms that prevented it from running when detecting foreign substances in the fuel. I modified the firmware to bypass these restrictions and allow sulfur-infused fuel to be burned.\n\n- Measuring Sulfur Dioxide Emissions:\n\nA sulfur dioxide sensor was placed in a copper tube behind the engine to monitor emissions and quantify how much sulfur was successfully converted from fuel to atmospheric aerosol precursors.\n\n- Thrust Performance Analysis:\n\nThe engine was mounted on a force dynamo, which measured changes in thrust output. Since fuel composition can impact engine efficiency, it was important to determine if adding sulfur affected performance.\n\n## Findings\n\nThe results confirmed the fundamental hypothesis: adding sulfur to the fuel led to increased sulfur dioxide emissions, making this a viable method for stratospheric aerosol injection. However, an interesting tradeoff emerged:\n\n- Small amounts of sulfur improved engine performance slightly while still generating the desired emissions.\n\n- Excessive sulfur content led to a decline in thrust output, which could make it difficult for an aircraft to reach the stratosphere.\n\nThis revealed a crucial optimization problem - balancing sulfur levels to maximize climate impact while maintaining efficient aircraft performance.\n\n## Implications & Future Research\n\nWhile SAI remains a high-risk, last-resort solution for climate change, my project demonstrated that passively integrating sulfur into jet fuel is a feasible approach to geoengineering. It eliminates the need for dedicated spraying infrastructure and aligns with existing aviation technology.\n\nOf course, serious risks remain. Any large-scale geoengineering effort requires extensive environmental modeling and international collaboration. History has shown that manipulating natural systems can have unintended consequences. Therefore, SAI should not be seen as an immediate solution, but rather as a potential tool for the future - one that could be deployed if climate conditions become dire enough to warrant it.\n\nAny time humans have messed with nature, we have had unintended consequences. Stories of humans releasing a certain species into the wild, and it becoming an invasive species, are a dime a dozen. And that's in a relatively simple system like an ecosystem. The atmosphere is a much more complex system, and so the potential for unintended consequences is much higher.\n\nHowever, I still think the climate threat, if left unchecked, is so severe that we should be exploring all options. As a last resort option to governments in the latter half of the century, SAI could be a crucial tool to save humanity.\n\nThis project was an exciting step toward understanding practical climate intervention methods. The next phase could involve further optimizing the fuel mixture, testing on larger jet engines, and assessing the long-term atmospheric impacts of such an approach. While geoengineering is not a silver bullet, it may one day provide a crucial buffer as humanity transitions to a sustainable energy future.\n\n    `,\n    coverImage: \"/geoeng.jpg\",\n    isProjectBlog: true,\n    projectId: 4,\n    tags: [\"Python\", \"Firmware\", \"Mechanical Design\", \"CAD\"]\n  },\n  {\n    id: \"5\",\n    title: \"reverseATS: find the most relevant job postings\",\n    slug: \"reverseats-project-blog\",\n    date: \"2025-03-19\",\n    summary: \"How I built a tool to find the most relevant job postings for a given resume\",\n    content: `\n    Finding a co-op is hard. Finding the right co-op is even harder.\n\nAs a University of Waterloo student, I've spent countless hours inside WaterlooWorks - our school's co-op job portal - scrolling through thousands of postings, unsure which ones were actually worth my time. Some jobs felt like a shot in the dark. Others looked promising but yielded no response. The whole process felt like I was playing darts blindfolded.\n\nIt got me thinking: recruiters use Applicant Tracking Systems (ATS) to screen candidates before they ever see a resume. These systems do keyword matching, similarity scoring, and automatic filtering. But as applicants, we're flying blind. We don't get to see how well we match a posting. We don't get a score. We don't even get feedback.\n\nSo I decided to flip the system.\n\n## Introducing ReverseATS\n\nReverseATS is a Chrome extension that analyzes every job listing on WaterlooWorks and ranks them based on how closely they match your resume. It's a reverse-engineered ATS - one that works for students instead of recruiters.\n\nUpload your resume, visit any page on WaterlooWorks, and the extension quietly scans the jobs, compares them against your experience using keyword similarity, and injects a match score next to each posting. No buttons. No clutter. Just seamless signal in a sea of noise.\n\n## Why I Built It\n\nThe core problem I wanted to solve was fit - helping students answer the question: What jobs am I most likely to get?\nIt's not about filtering only the “best” jobs. It's about visibility. If I knew I was a 92% match for Job A but only a 31% match for Job B, I could make better decisions. I could prioritize jobs I had a shot at. I could tailor my applications more effectively. I could stop wasting time.\n\nReverseATS brings that insight to the surface. It makes the job hunt feel less like roulette and more like chess.\n\n## The Technical Journey (and a Mid-Project Meltdown)\n\nReverseATS started as a simple scraping tool. WaterlooWorks used to have predictable, REST-style URLs and static HTML content. I used Python, BeautifulSoup, and a bunch of requests logic to pull job descriptions and analyze them. It was crude but effective.\n\nThen - disaster.\n\nAbout 15 days into development, just as I was wrapping up, WaterlooWorks pushed a full UI redesign. The entire front-end architecture changed. No more simple HTML. No more direct URLs. Instead, job data was now fetched through a hidden API gated by dynamically generated action tokens.\n\nI had to reverse-engineer their new system. This meant:\n\t•\tDigging into their front-end JavaScript bundle\n\t•\tLocating the function responsible for generating the token\n\t•\tParsing obfuscated API calls to understand how requests were signed\n\t•\tExtracting session cookies and mimicking the browser's authenticated state\n\nIn short, it went from a weekend project to a deep-dive into front-end forensics.\n\nEventually, I rebuilt the extension from scratch. This time, instead of HTML scraping, ReverseATS:\n\t1.\tAuthenticates via the user's browser session (grabbing cookies and tokens after login)\n\t2.\tFetches job listings in batches using WaterlooWorks' hidden API\n\t3.\tParses job descriptions and metadata\n\t4.\tApplies a keyword similarity algorithm between your resume and each job\n\t5.\tInjects match scores directly into the UI - no extra clicks, no context switching\n\nAll of it happens live, on the page you're already on.\n\n## Why It Matters\n\nIf you're a Waterloo student, you've probably felt the fatigue of applying to dozens of jobs with no clear strategy. ReverseATS gives you a compass. It doesn't guarantee a job - but it does help you navigate smarter.\n\nMy hope is that it saves students time, reduces stress, and helps people apply more strategically. The co-op system has its challenges, but that doesn't mean we have to go in blind.\n\n## What's Next?\n\nThere are a lot of potential improvements: resume parsing from PDF, smarter similarity metrics, a dashboard for tracking matches over time, maybe even automated cover letter generation based on matched keywords.\n\nBut for now, I'm just excited to get this into the hands of other students.\n\nIf you want to try ReverseATS, you can download it [here](https://chromewebstore.google.com/detail/ReverseATS%20Job%20Matcher/ipkldjngbilnepdikdjmhjhfagbjllnj).\n    `,\n    coverImage: \"/reverseats.png\",\n    isProjectBlog: true,\n    projectId: 5,\n    tags: [\"JavaScript\", \"Python\", \"BeautifulSoup\", \"Chrome Extension\"]\n  },\n  {\n    id: \"6\",\n    title: \"chatUW: A Chatbot Trained on Waterloo Student Wisdom\",\n    slug: \"chatuw-project-blog\",\n    date: \"2025-04-05\",\n    summary: \"How I built a RAG chatbot for the University of Waterloo\",\n    content: `\n\nIf you've spent more than five minutes in any University of Waterloo Discord server, Reddit thread, or group chat, you know one thing's for sure: Waterloo students love writing guides.\n\nThere are guides for everything - co-op, surviving first year, U.S. immigration, prof rankings, housing, resumes, entrepreneurship, food, even memes. The sheer volume is honestly impressive.\n\nBut here's the problem: most of them get lost in the noise.\n\nThey're buried in outdated Google Docs, scattered across subreddits, or sitting idle in random Notion pages. New students never find them. Older students forget about them. And most of the content, while incredibly useful, ends up underutilized.\n\nThat got me thinking:\nWhat if you could access all that wisdom instantly?\nWhat if there was a chatbot trained on all those student-made guides - a single place you could go to ask anything about student life at Waterloo?\n\n## Introducing chatUW\n\nchatUW is a RAG-based (Retrieval-Augmented Generation) chatbot trained on publicly available Waterloo student guides. It combines everything from co-op and U.S. visa guides to restaurant recommendations and housing tips into one seamless conversational interface.\n\nIt's basically the student version of ChatGPT - but trained on Waterloo-specific life advice, experience, and resources.\n\nYou can ask it questions like:\n\t•\t“What's the process for getting a co-op in the U.S.?”\n\t•\t“Where are some cheap places to eat around campus?”\n\t•\t“What are some tips for first-year CS students?”\n\t•\t“How do I deal with terrible landlords in Waterloo?”\n\t•\t“What's the difference between Stream 4 and Stream 8?”\n\nAnd it'll give you helpful, informed answers based on real guides made by real students.\n\n## Why I Built It\n\nThere was no shortage of content. What was missing was accessibility. The problem wasn't that students weren't sharing knowledge - it was that their knowledge wasn't discoverable or usable at scale.\n\nWe have an informal search engine already - asking upper-years in Discord servers, or sifting through Reddit threads. But that process is inefficient. You might ask a question and get 10 conflicting answers. Or worse, no response at all.\n\nchatUW is meant to be the always-on version of that. It's the “student who's been here for five years and has seen it all” - just in chatbot form.\n\n## What's Under the Hood\n\nThe stack is pretty standard for a RAG LLM app:\n\t•\tFrontend: Built with Next.js\n\t•\tSecurity: Google reCAPTCHA to prevent abuse\n\t•\tVector Store: Pinecone for fast, scalable embedding search\n\t•\tLLM: OpenAI GPT API\n\t•\tRetrieval: LangChain-powered RAG pipeline to fetch relevant student guide chunks based on your query\n\nThe knowledge base was built by scraping and parsing publicly available guides, including survival guides, co-op advice docs, immigration FAQs, and more. Everything included is open-source or explicitly shared by students - nothing proprietary or copyrighted by the university itself.\n\n## Why It Matters\n\nWaterloo can be overwhelming. The systems are complex, the processes are confusing, and no two students take the same path. But there's a ton of collective wisdom floating around - it just needed a place to live.\n\nchatUW gives students an accessible, conversational way to tap into that collective knowledge. Whether you're new to campus or about to graduate, it's designed to help you navigate student life smarter and faster.\n\n## What's Next?\n\nI'm planning to:\n\t•\tAdd document sources so users can trace where answers came from\n\t•\tAllow uploading of your own guide to contribute to the corpus\n\t•\tIntegrate better UI/UX features to make chatUW feel more like a real student peer than just another bot\n\nThis started as a small side project, but it's quickly becoming something I wish I had when I started at Waterloo.\n\nIf you want to try chatUW or contribute to the guide corpus, reach out - I'd love to chat.\n    `,\n    coverImage: \"/chatuw.png\",\n    isProjectBlog: true,\n    projectId: 6,\n    tags: [\"Pinecone\", \"OpenAI\", \"Next.js\"]\n  }\n];\n\nexport async function getBlogBySlug(slug: string): Promise<BlogPost | undefined> {\n  return blogPosts.find(post => post.slug === slug);\n}\n\nexport async function getProjectBlog(projectId: number): Promise<BlogPost | undefined> {\n  return blogPosts.find(post => post.isProjectBlog && post.projectId === projectId);\n}\n\nexport async function getAllBlogs(): Promise<BlogPost[]> {\n  return blogPosts;\n}\n\nexport async function getNonProjectBlogs(): Promise<BlogPost[]> {\n  return blogPosts.filter(post => !post.isProjectBlog);\n}\n\nexport async function getProjectBlogs(): Promise<BlogPost[]> {\n  return blogPosts.filter(post => post.isProjectBlog);\n} "],"names":[],"mappings":";;;;;;;;AAaO,MAAM,YAAwB;IACnC;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAkCV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAO;YAAgB;YAAmB;SAAU;IAC7D;IACA;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IA0CV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAQ;YAAc;SAAS;IACxC;IACA;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAsDV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAc;YAAY;YAAO;SAAS;IACnD;IACA;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IA6DV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAU;YAAY;YAAqB;SAAM;IAC1D;IACA;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IA4DV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAc;YAAU;YAAiB;SAAmB;IACrE;IACA;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAgEV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAY;YAAU;SAAU;IACzC;CACD;AAEM,eAAe,cAAc,IAAY;IAC9C,OAAO,UAAU,IAAI,CAAC,CAAA,OAAQ,KAAK,IAAI,KAAK;AAC9C;AAEO,eAAe,eAAe,SAAiB;IACpD,OAAO,UAAU,IAAI,CAAC,CAAA,OAAQ,KAAK,aAAa,IAAI,KAAK,SAAS,KAAK;AACzE;AAEO,eAAe;IACpB,OAAO;AACT;AAEO,eAAe;IACpB,OAAO,UAAU,MAAM,CAAC,CAAA,OAAQ,CAAC,KAAK,aAAa;AACrD;AAEO,eAAe;IACpB,OAAO,UAAU,MAAM,CAAC,CAAA,OAAQ,KAAK,aAAa;AACpD"}},
    {"offset": {"line": 447, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 531, "column": 0}, "map": {"version":3,"sources":["file:///Users/aryan/projects/personalwebsite/app/blogs/%5Bslug%5D/page.tsx"],"sourcesContent":["import { getBlogBySlug, getAllBlogs } from '@/data/blogs';\nimport { notFound } from 'next/navigation';\nimport Image from 'next/image';\nimport Link from 'next/link';\nimport { Metadata } from 'next';\n\n// Add markdown rendering\nimport ReactMarkdown from 'react-markdown';\n\n// Define the params type for this page\ntype BlogPostParams = {\n  slug: string;\n};\n\n// Generate metadata for the page\nexport async function generateMetadata({ \n  params \n}: { \n  params: Promise<BlogPostParams> \n}): Promise<Metadata> {\n  const resolvedParams = await params;\n  const slug = resolvedParams.slug;\n  const blog = await getBlogBySlug(slug);\n  \n  if (!blog) {\n    return {\n      title: 'Blog Post Not Found',\n    };\n  }\n  \n  return {\n    title: `${blog.title} | Aryan Gupta`,\n    description: blog.summary,\n  };\n}\n\n// Generate static paths for all blog posts\nexport async function generateStaticParams() {\n  const blogs = await getAllBlogs();\n  \n  return blogs.map((blog) => ({\n    slug: blog.slug,\n  }));\n}\n\nexport default async function BlogPostPage({ \n  params \n}: { \n  params: Promise<BlogPostParams> \n}) {\n  const resolvedParams = await params;\n  const slug = resolvedParams.slug;\n  const blog = await getBlogBySlug(slug);\n  \n  if (!blog) {\n    notFound();\n  }\n  \n  return (\n    <main className=\"min-h-screen bg-black text-white\">\n      <div className=\"max-w-4xl mx-auto px-4 py-28 sm:px-6 lg:px-8\">\n        <Link href=\"/blogs\" className=\"inline-flex items-center text-blue-400 hover:text-blue-300 mb-8\">\n          <svg xmlns=\"http://www.w3.org/2000/svg\" className=\"h-5 w-5 mr-2\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n            <path fillRule=\"evenodd\" d=\"M9.707 16.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 011.414 1.414L5.414 9H17a1 1 0 110 2H5.414l4.293 4.293a1 1 0 010 1.414z\" clipRule=\"evenodd\" />\n          </svg>\n          Back to all blogs\n        </Link>\n        \n        <article className=\"prose prose-invert prose-lg max-w-none\">\n          <header className=\"mb-10 not-prose\">\n            <div className=\"flex items-center text-sm text-gray-400 mb-4\">\n              <time dateTime={blog.date}>{new Date(blog.date).toLocaleDateString('en-US', {\n                year: 'numeric',\n                month: 'long',\n                day: 'numeric'\n              })}</time>\n              \n              {blog.isProjectBlog && (\n                <>\n                  <span className=\"mx-2\">•</span>\n                  <span className=\"text-blue-400\">Project Blog</span>\n                </>\n              )}\n            </div>\n            \n            <h1 className=\"text-4xl font-bold mb-6 glow-text\">{blog.title}</h1>\n            \n            <p className=\"text-xl text-gray-300 mb-6\">{blog.summary}</p>\n            \n            <div className=\"flex flex-wrap gap-2 mb-8\">\n              {blog.tags.map((tag) => (\n                <span key={tag} className=\"px-3 py-1 text-sm bg-gray-800 rounded-full text-gray-300\">\n                  {tag}\n                </span>\n              ))}\n            </div>\n            \n            {blog.coverImage && (\n              <div className=\"aspect-w-16 aspect-h-9 relative overflow-hidden rounded-lg mb-10 shadow-lg\">\n                <Image\n                  src={blog.coverImage}\n                  alt={blog.title}\n                  fill\n                  className=\"object-cover\"\n                  priority\n                />\n              </div>\n            )}\n          </header>\n          \n          <div className=\"markdown-content\">\n            <ReactMarkdown>{blog.content}</ReactMarkdown>\n          </div>\n        </article>\n      </div>\n    </main>\n  );\n} "],"names":[],"mappings":";;;;;;AAAA;AACA;AACA;AACA;AAFA;AAKA,yBAAyB;AACzB;;;;;;;AAQO,eAAe,iBAAiB,EACrC,MAAM,EAGP;IACC,MAAM,iBAAiB,MAAM;IAC7B,MAAM,OAAO,eAAe,IAAI;IAChC,MAAM,OAAO,MAAM,CAAA,GAAA,oHAAA,CAAA,gBAAa,AAAD,EAAE;IAEjC,IAAI,CAAC,MAAM;QACT,OAAO;YACL,OAAO;QACT;IACF;IAEA,OAAO;QACL,OAAO,GAAG,KAAK,KAAK,CAAC,cAAc,CAAC;QACpC,aAAa,KAAK,OAAO;IAC3B;AACF;AAGO,eAAe;IACpB,MAAM,QAAQ,MAAM,CAAA,GAAA,oHAAA,CAAA,cAAW,AAAD;IAE9B,OAAO,MAAM,GAAG,CAAC,CAAC,OAAS,CAAC;YAC1B,MAAM,KAAK,IAAI;QACjB,CAAC;AACH;AAEe,eAAe,aAAa,EACzC,MAAM,EAGP;IACC,MAAM,iBAAiB,MAAM;IAC7B,MAAM,OAAO,eAAe,IAAI;IAChC,MAAM,OAAO,MAAM,CAAA,GAAA,oHAAA,CAAA,gBAAa,AAAD,EAAE;IAEjC,IAAI,CAAC,MAAM;QACT,CAAA,GAAA,qLAAA,CAAA,WAAQ,AAAD;IACT;IAEA,qBACE,8OAAC;QAAK,WAAU;kBACd,cAAA,8OAAC;YAAI,WAAU;;8BACb,8OAAC,4JAAA,CAAA,UAAI;oBAAC,MAAK;oBAAS,WAAU;;sCAC5B,8OAAC;4BAAI,OAAM;4BAA6B,WAAU;4BAAe,SAAQ;4BAAY,MAAK;sCACxF,cAAA,8OAAC;gCAAK,UAAS;gCAAU,GAAE;gCAAwI,UAAS;;;;;;;;;;;wBACxK;;;;;;;8BAIR,8OAAC;oBAAQ,WAAU;;sCACjB,8OAAC;4BAAO,WAAU;;8CAChB,8OAAC;oCAAI,WAAU;;sDACb,8OAAC;4CAAK,UAAU,KAAK,IAAI;sDAAG,IAAI,KAAK,KAAK,IAAI,EAAE,kBAAkB,CAAC,SAAS;gDAC1E,MAAM;gDACN,OAAO;gDACP,KAAK;4CACP;;;;;;wCAEC,KAAK,aAAa,kBACjB;;8DACE,8OAAC;oDAAK,WAAU;8DAAO;;;;;;8DACvB,8OAAC;oDAAK,WAAU;8DAAgB;;;;;;;;;;;;;;8CAKtC,8OAAC;oCAAG,WAAU;8CAAqC,KAAK,KAAK;;;;;;8CAE7D,8OAAC;oCAAE,WAAU;8CAA8B,KAAK,OAAO;;;;;;8CAEvD,8OAAC;oCAAI,WAAU;8CACZ,KAAK,IAAI,CAAC,GAAG,CAAC,CAAC,oBACd,8OAAC;4CAAe,WAAU;sDACvB;2CADQ;;;;;;;;;;gCAMd,KAAK,UAAU,kBACd,8OAAC;oCAAI,WAAU;8CACb,cAAA,8OAAC,6HAAA,CAAA,UAAK;wCACJ,KAAK,KAAK,UAAU;wCACpB,KAAK,KAAK,KAAK;wCACf,IAAI;wCACJ,WAAU;wCACV,QAAQ;;;;;;;;;;;;;;;;;sCAMhB,8OAAC;4BAAI,WAAU;sCACb,cAAA,8OAAC,wLAAA,CAAA,UAAa;0CAAE,KAAK,OAAO;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAMxC"}},
    {"offset": {"line": 746, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 757, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":""}},
    {"offset": {"line": 757, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}}]
}