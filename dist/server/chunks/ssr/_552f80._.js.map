{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 7, "column": 0}, "map": {"version":3,"sources":["file:///Users/aryan/projects/personalwebsite/src/data/blogs.ts"],"sourcesContent":["export type BlogPost = {\n  id: string;\n  title: string;\n  slug: string;\n  date: string;\n  summary: string;\n  content: string;\n  coverImage?: string;\n  isProjectBlog: boolean;\n  projectId?: number;\n  tags: string[];\n};\n\nexport const blogPosts: BlogPost[] = [\n  {\n    id: \"1\",\n    title: \"Hyperloo: Building a Knowledge Graph for University Courses\",\n    slug: \"hyperloo-project-blog\",\n    date: \"2025-04-23\",\n    summary: \"How I built an algorithmically generated knowledge graph for UWaterloo programs and courses\",\n    content: `\nHyperloo is a knowledge graph that maps out all topics, courses, and degrees at the University of Waterloo. Traditional degree structures tend to be abstract, making it difficult to visualize the interconnected nature of knowledge. I wanted to change that. What if you could represent an entire degree visually - showing how concepts interlink and build on one another? That question led me deep into knowledge graphs, which I found incredibly powerful as a tool for structuring and exploring complex domains.\n\n## Why Knowledge Graphs?\n\nConcepts are inherently nested objects. Take nuclear physics - understanding the entire field seems daunting. But break it down, and it's really just a collection of interconnected subtopics. Each of those subtopics can be further divided, creating a layered structure that makes even the most complex subjects feel approachable. Knowledge graphs embody this philosophy: no topic is truly out of reach if broken down correctly. Seeing how subjects connect makes learning more intuitive, empowering students to explore topics they once thought were beyond them.\n\nOne of my key inspirations was the Socratica graph matching from the 2023 Socratica Symposium. As graph tooling continues to improve, I believe we'll see knowledge graphs become a standard way to represent and navigate information.\n\n## Building Hyperloo\n\nCreating Hyperloo was a long and technically challenging process. The first step was data collection: scraping every Waterloo syllabus to form a base knowledge corpus. We used Selenium to automate the scraping, parsing programs, courses, and subtopics iteratively. This required multiple browser automation scripts to extract structured data from unstructured web pages. The process was labor-intensive, but necessary.\n\nOnce we had a structured corpus, the next challenge was transforming raw syllabus text into meaningful graph data. We trained a custom NLP model using SpaCy to extract key information. The NLP pipeline was relatively simple - a classification model trained with labeled examples to recognize important syllabus components. After multiple iterations, we achieved ~92% accuracy, which was sufficient for our needs. The model's purpose wasn't perfect precision but rather rough approximation to filter syllabus content into useful knowledge nodes.\n\nWith the extracted information, we structured the data into a nested JSON format that could be easily visualized as a graph. To generate these structured JSON objects, we used OpenAI's Batch API, processing large volumes of text and distilling them into a structured hierarchy. The result was a massive JSON-L file representing Waterloo's academic knowledge as a network of interconnected topics.\n\nFinally, we built the front end using React, leveraging the GraphForce component to render the knowledge graph. While some customization was required to optimize the visualization, the hardest part of the project was the data transformation itself - getting from raw syllabi to structured knowledge nodes.\n\n## Impact and Utility\n\nHyperloo has already gained traction. After sharing it on LinkedIn and Twitter, it received over 250 likes on LinkedIn and 100+ on Twitter. More importantly, it was bookmarked 17 times on Twitter - an indicator that people actually intend to use it as a reference.\n\nFor Waterloo students, the utility is clear. Hyperloo provides a structured, visual way to explore degree programs, understand prerequisite relationships, and dive into any topic of interest. Even if only a few dozen students actively use it, that's a meaningful outcome for me.\n\nBut the implications go beyond Waterloo. With Hyperloo, anyone, anywhere in the world, can effectively trace the structure of a Waterloo degree and use it as a self-learning roadmap. Even though it's not a complete curriculum, the ability to map out an entire field and navigate it freely is incredibly powerful. In theory, a student in a third world nation or any remote region could use Hyperloo, coupled with Perplexity AI and other online resources, to pursue an entire degree's worth of knowledge for free.\n\n## What's Next?\n\nBefore starting any project, I ask myself: if I were to disappear tomorrow, what would I leave behind? Hyperloo is one of those projects that feels genuinely useful - not just to me, but to the broader world. If it grows, it could be a foundational resource for structured, open-access education. That's the kind of impact worth building for.\n\nCheck out the production version of Hyperloo [here](https://tumph.github.io/hyperlooprod/).\n\n\n    `,\n    coverImage: \"/hyperloo.png\",\n    isProjectBlog: true,\n    projectId: 1,\n    tags: [\"NLP\", \"Web Scraping\", \"Knowledge Graph\", \"Next.js\"]\n  },\n  {\n    id: \"2\",\n    title: \"Doledesk: Automating Substitute Teacher Scheduling\",\n    slug: \"doledesk-project-blog\",\n    date: \"2023-03-15\",\n    summary: \"How I built a system to automate substitute teacher scheduling\",\n    content: `\nIn 2023, I started looking into inefficiencies in school operations - areas where automation could replace tedious, repetitive processes. Automation was becoming increasingly practical, and I wanted to build something with real impact. That led me to a conversation with my high school's vice principal. I asked a simple question: “What are the most time-consuming tasks you deal with daily?”\n\nThe answer was clear: substitute teacher scheduling. Every morning, administrators scrambled to check which teachers were absent, cross-referencing a massive spreadsheet to manually assign substitutes. It was an inefficient, error-prone process - one that was ripe for automation. That insight led me to build Doledesk, a system designed to fully automate substitute teacher scheduling using a rules-based algorithm.\n\n## The Complexity of Scheduling\n\nAt first, scheduling substitutes seemed like a straightforward problem - identify absent teachers and match them with available subs. But the deeper I went, the more complexity I uncovered. Several constraints made this a non-trivial problem:\n\n- Legal Compliance: Many school districts have strict labor laws. For example, a teacher can't work more than three consecutive periods without a break.\n\n- Subject Matching: Not all substitutes can teach every subject. A math teacher shouldn't be assigned to an English class.\n\n- Multi-Layered Dependencies: If one substitute isn't available, the entire schedule may need to be reshuffled dynamically.\n\n- Data Privacy Restrictions: Storing teacher data required careful adherence to K-12 privacy regulations, meaning traditional database solutions weren't viable.\n\nThese constraints required a robust, flexible system capable of handling real-world edge cases while optimizing for efficiency.\n\n## Building Doledesk\n\nThe core of Doledesk was a backend system built with Java and JavaScript, designed to process teacher absences and dynamically assign substitutes. Here's how it worked:\n\n- Data Input Pipeline: Each morning, a fresh list of absent teachers was fed into the system. Since storing persistent data wasn't an option due to privacy regulations, all scheduling had to happen in real-time.\n\n- Algorithmic Matching: The backend used a constraint-satisfaction algorithm to assign substitutes based on availability, subject expertise, and legal guidelines. If an optimal match wasn't found, the algorithm recursively adjusted placements.\n\n- Automated Notifications: Once schedules were finalized, the system sent out automated email notifications to substitutes and teachers.\n\n- Failsafe Mechanisms: If any substitute declined their assignment, the system reran the matching algorithm to fill gaps dynamically.\n\nFor the UI, I opted for Bubble, a no-code editor, to accelerate frontend development. This allowed me to quickly iterate on the UI and get a working product.\n\n## Deployment and Impact\n\nOnce Doledesk was live, my school used it for a month. The result? Administrators who previously spent hours manually assigning substitutes were now completing the process in minutes. What used to be a chaotic, last-minute scramble was now a structured, automated workflow.\n\nWhile this project started as an experiment, it quickly became something more - proof that automation can significantly reduce administrative overhead in education. Doledesk wasn't just about saving time; it was about ensuring that students always had the right teachers in place, improving the overall classroom experience.\n\n## What's Next?\n\nDoledesk validated an important idea: many outdated, manual processes in education can be automated with the right approach. Looking forward, I see opportunities to expand this concept beyond substitute scheduling - perhaps into broader school operations or even district-wide automation tools.\n    `,\n    coverImage: \"/doledesk.png\",\n    isProjectBlog: true,\n    projectId: 2,\n    tags: [\"Java\", \"JavaScript\", \"Bubble\"]\n  },\n  {\n    id: \"3\",\n    title: \"Pare: Summarizing Resumes on Any ATS\",\n    slug: \"pare-project-blog\",\n    date: \"2024-09-3\",\n    summary: \"How I built a browser extension to summarize resumes on any ATS\",\n    content: `\n## The Hiring Challenge\n\nIn 2024, I embarked on a project to tackle inefficiencies in the recruiting industry, a journey inspired by my experience with the Waterloo Co-op program. My first co-op was at an AI company called ada, and while the opportunity was exciting, the hiring process itself was frustratingly arduous. Securing the position required significant effort, and the lack of feedback or even rejection emails while applying to other jobs left me feeling like my resume was getting lost in the void.\n\nAt first, I blamed recruiters, assuming they weren't reviewing resumes properly. However, after speaking with several recruiters, I realized the problem wasn't them - it was the sheer volume of applications they had to process daily. Some recruiters I spoke to had thousands of resumes to sift through, often spending only 5-10 seconds per resume. Given these constraints, it became clear that recruiters simply didn't have the bandwidth to provide personalized responses to every applicant.\n\nThis insight led me to ask: Could technology streamline this process and help recruiters make better decisions, faster? That's when I built Pare, a resume summarization tool designed to make the recruiting process more efficient.\n\n## Introducing Pare: A Resume Summarizer for ATS Platforms\n\nPare is a browser extension that integrates with any Applicant Tracking System (ATS) to help recruiters process resumes faster. For those unfamiliar, an ATS is the software recruiters use to track applicants, manage job postings, and oversee hiring pipelines - it's like CRM software but tailored for recruiting.\n\nSince recruiters only have a few seconds to assess a resume, I saw an opportunity to reduce cognitive load by providing structured, AI-generated summaries. The goal was simple: help recruiters get to the essence of a candidate's experience in a fraction of the time.\n\n## The Technical Approach\n\nBuilding Pare came with significant technical challenges. The primary complexity lay in making the extension work seamlessly across multiple ATS platforms, each with its own data structures and UI implementations. Some ATS platforms rely on React components, others use HTML pop-ups, and some simply display plain text.\n\nHere's how Pare works:\n\n- Resume Extraction: Pare uses PDF.js to extract the text from resumes displayed in ATS platforms.\n\n- AI-Powered Summarization: The extracted resume text is then fed into OpenAI's GPT model with structured prompting to generate a concise, recruiter-friendly summary.\n\n- Dynamic Integration: The extension identifies different ATS architectures and adapts accordingly to display the summarized resume in the appropriate section of the UI.\n\n## Overcoming Edge Cases\n\nA key challenge in developing Pare was ensuring its functionality across different ATS platforms. Some ATS systems store resume data in embedded JavaScript objects, while others use dynamically generated iframes. To address these variations, Pare includes:\n\n- DOM Inspection and Adaptation: The extension dynamically detects how the ATS renders resumes and adapts its extraction process accordingly.\n\n- Asynchronous Handling: Since ATS platforms load data asynchronously, Pare waits for DOM elements to fully render before extracting content, preventing errors from incomplete data.\n\n- Cross-Origin Requests: Some ATS platforms restrict direct access to resume content, requiring workarounds like injecting scripts into the page context to retrieve the necessary data.\n\n## Adoption and Impact\n\nSince launching Pare, several recruiters - many of whom I connected with during the development process - have started using it. A few of my friends running startups with high hiring volumes have also adopted it to streamline their recruiting workflows.\n\nThe extension is available on GitHub, where users can configure it with their own OpenAI API key and install it as an unpacked Chrome extension via Developer Tools. By reducing the time recruiters spend reading resumes, Pare helps them focus on identifying the best candidates rather than drowning in an overwhelming sea of applications.\n\n## What's Next?\n\nPare is just the beginning. There are still many inefficiencies in the hiring process that can be optimized with AI-driven tools. Future iterations of Pare could incorporate:\n\n- Customizable Summarization Styles: Allowing recruiters to tweak summary formats based on industry-specific needs.\n\n- Multi-Resume Comparisons: Automatically highlighting key differentiators between candidates.\n\n- Integration with More ATS Platforms: Expanding native support for widely used recruiting systems.\n\nAt its core, Pare is about making hiring more efficient - both for recruiters and job seekers. By leveraging AI to handle tedious, high-volume tasks, recruiters can spend more time on what truly matters: connecting great candidates with great opportunities.\n    `,\n    coverImage: \"/pare.png\",\n    isProjectBlog: true,\n    projectId: 3,\n    tags: [\"JavaScript\", \"HTML/CSS\", \"Git\", \"PDF.js\"]\n  },\n  {\n    id: \"4\",\n    title: \"SAI Microjet: Optimizing Sulfur Ratios for Geoengineering\",\n    slug: \"sai-microjet-project-blog\",\n    date: \"2022-07-09\",\n    summary: \"Developing a hardware rig and firmware system for a microjet engine to test optimal sulfur ratios for stratospheric aerosol injection.\",\n    content: `\n## A Climate Engineering Experiment\n\nClimate change is one of the most pressing challenges of our time. At its core, it is an energy problem - greenhouse gases trap more solar radiation in the Earth's atmosphere, increasing surface temperatures.\n\nIf you think about climate change from a physics perspective - Earth is essentially a self contained system (all the energy that arrives on Earth from the sun gets dispersed in some way or form - energy cannot be created or destroyed!) and so a thicker atmosphere with higher PPMs of greenhouse gases that trap energy more effectively would lead to more of that energy being dispersed as heat in the atmosphere instead of staying as radiation being reflected back into space. \n\nCurrent efforts to mitigate climate change generally fall into two categories: reducing carbon emissions (by transitioning to renewable energy) and removing carbon already in the atmosphere (through carbon capture technologies). However, this ignores the fact that from a physics perspective, if you want to reduce the energy trapped in a system - you could increase permabilility of the system so less energy is trapped (current methods) - but you could also just prevent that energy from reaching the system in the first place.\n\nGeoengineering strategies aim to reduce the amount of solar radiation reaching Earth. Among the various proposals - such as placing giant mirrors in space or brightening marine clouds - one of the most promising and realistic methods (not as pie in the sky as the other ones) is stratospheric aerosol injection (SAI). This technique involves releasing small aerosol particles, such as sulfur compounds, into the stratosphere to reflect sunlight and cool the planet, similar to how volcanic eruptions impact global temperatures. The reason this is the most realistic is this has actually happened naturally before - A historical example of this effect was the 1991 eruption of Mount Pinatubo, which led to a temporary global cooling of 1-2 degrees Celsius.\n\nEven if you reduce the amount of sunlight reaching Earth by 1-2%, you would completely negate the 2-3 centuries of anthropogenic climate change we have experienced up till today. Isn't that crazy? And the effect to plant fauna would be negligible - would you really be thirsty if your glass of water had 1% less water in it?\n\n## A Practical Approach: Sulfur Injection via Jet Engine Fuel\n\nRather than deploying a specialized fleet of aircraft carrying sulfur dioxide tanks for stratospheric aerosol injection (as current research is doing), my project explored a more efficient approach: integrating sulfur directly into jet fuel. This concept allows aircraft to release sulfur precursors passively as they fly, eliminating the need for dedicated spraying equipment.\n\nMy research, conducted with Andrew Lockley from the University College London, began with a conceptual analysis of this approach. However, to validate the feasibility of sulfur-infused jet fuel, I moved beyond theory into practical experimentation.\n\n## Testing with a Microjet Engine\n\nTo test the concept, I acquired a microjet engine - a scaled-down jet engine that operates on the same physical principles as full-scale aircraft turbines. Instead of experimenting on a multimillion-dollar Rolls-Royce engine, a microjet provided a cost-effective, low-risk way to evaluate fuel modifications.\n\n## Experiment Design\n\nThe experiment involved three key components:\n\n- Engine Modification & Sensor Integration:\n\nThe microjet engine had built-in safety mechanisms that prevented it from running when detecting foreign substances in the fuel. I modified the firmware to bypass these restrictions and allow sulfur-infused fuel to be burned.\n\n- Measuring Sulfur Dioxide Emissions:\n\nA sulfur dioxide sensor was placed in a copper tube behind the engine to monitor emissions and quantify how much sulfur was successfully converted from fuel to atmospheric aerosol precursors.\n\n- Thrust Performance Analysis:\n\nThe engine was mounted on a force dynamo, which measured changes in thrust output. Since fuel composition can impact engine efficiency, it was important to determine if adding sulfur affected performance.\n\n## Findings\n\nThe results confirmed the fundamental hypothesis: adding sulfur to the fuel led to increased sulfur dioxide emissions, making this a viable method for stratospheric aerosol injection. However, an interesting tradeoff emerged:\n\n- Small amounts of sulfur improved engine performance slightly while still generating the desired emissions.\n\n- Excessive sulfur content led to a decline in thrust output, which could make it difficult for an aircraft to reach the stratosphere.\n\nThis revealed a crucial optimization problem - balancing sulfur levels to maximize climate impact while maintaining efficient aircraft performance.\n\n## Implications & Future Research\n\nWhile SAI remains a high-risk, last-resort solution for climate change, my project demonstrated that passively integrating sulfur into jet fuel is a feasible approach to geoengineering. It eliminates the need for dedicated spraying infrastructure and aligns with existing aviation technology.\n\nOf course, serious risks remain. Any large-scale geoengineering effort requires extensive environmental modeling and international collaboration. History has shown that manipulating natural systems can have unintended consequences. Therefore, SAI should not be seen as an immediate solution, but rather as a potential tool for the future - one that could be deployed if climate conditions become dire enough to warrant it.\n\nAny time humans have messed with nature, we have had unintended consequences. Stories of humans releasing a certain species into the wild, and it becoming an invasive species, are a dime a dozen. And that's in a relatively simple system like an ecosystem. The atmosphere is a much more complex system, and so the potential for unintended consequences is much higher.\n\nHowever, I still think the climate threat, if left unchecked, is so severe that we should be exploring all options. As a last resort option to governments in the latter half of the century, SAI could be a crucial tool to save humanity.\n\nThis project was an exciting step toward understanding practical climate intervention methods. The next phase could involve further optimizing the fuel mixture, testing on larger jet engines, and assessing the long-term atmospheric impacts of such an approach. While geoengineering is not a silver bullet, it may one day provide a crucial buffer as humanity transitions to a sustainable energy future.\n\n    `,\n    coverImage: \"/geoeng.jpg\",\n    isProjectBlog: true,\n    projectId: 4,\n    tags: [\"Python\", \"Firmware\", \"Mechanical Design\", \"CAD\"]\n  }\n];\n\nexport async function getBlogBySlug(slug: string): Promise<BlogPost | undefined> {\n  return blogPosts.find(post => post.slug === slug);\n}\n\nexport async function getProjectBlog(projectId: number): Promise<BlogPost | undefined> {\n  return blogPosts.find(post => post.isProjectBlog && post.projectId === projectId);\n}\n\nexport async function getAllBlogs(): Promise<BlogPost[]> {\n  return blogPosts;\n}\n\nexport async function getNonProjectBlogs(): Promise<BlogPost[]> {\n  return blogPosts.filter(post => !post.isProjectBlog);\n}\n\nexport async function getProjectBlogs(): Promise<BlogPost[]> {\n  return blogPosts.filter(post => post.isProjectBlog);\n} "],"names":[],"mappings":";;;;;;;;AAaO,MAAM,YAAwB;IACnC;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAkCV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAO;YAAgB;YAAmB;SAAU;IAC7D;IACA;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IA0CV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAQ;YAAc;SAAS;IACxC;IACA;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAsDV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAc;YAAY;YAAO;SAAS;IACnD;IACA;QACE,IAAI;QACJ,OAAO;QACP,MAAM;QACN,MAAM;QACN,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IA6DV,CAAC;QACD,YAAY;QACZ,eAAe;QACf,WAAW;QACX,MAAM;YAAC;YAAU;YAAY;YAAqB;SAAM;IAC1D;CACD;AAEM,eAAe,cAAc,IAAY;IAC9C,OAAO,UAAU,IAAI,CAAC,CAAA,OAAQ,KAAK,IAAI,KAAK;AAC9C;AAEO,eAAe,eAAe,SAAiB;IACpD,OAAO,UAAU,IAAI,CAAC,CAAA,OAAQ,KAAK,aAAa,IAAI,KAAK,SAAS,KAAK;AACzE;AAEO,eAAe;IACpB,OAAO;AACT;AAEO,eAAe;IACpB,OAAO,UAAU,MAAM,CAAC,CAAA,OAAQ,CAAC,KAAK,aAAa;AACrD;AAEO,eAAe;IACpB,OAAO,UAAU,MAAM,CAAC,CAAA,OAAQ,KAAK,aAAa;AACpD"}},
    {"offset": {"line": 290, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 296, "column": 0}, "map": {"version":3,"sources":["file:///Users/aryan/projects/personalwebsite/app/blogs/page.tsx"],"sourcesContent":["import Link from 'next/link';\nimport Image from 'next/image';\nimport { getAllBlogs } from '@/data/blogs';\n\nexport const metadata = {\n  title: 'Blogs | Aryan Gupta',\n  description: 'Blog posts about technology, projects, and more by Aryan Gupta',\n};\n\nexport default async function BlogsPage() {\n  const blogs = await getAllBlogs();\n  \n  return (\n    <main className=\"min-h-screen bg-black text-white\">\n      <div className=\"max-w-5xl mx-auto px-4 py-28 sm:px-6 lg:px-8\">\n        <h1 className=\"text-4xl font-bold mb-8 tracking-tight\">Blog</h1>\n        \n        <div className=\"grid gap-10\">\n          {blogs.map((blog) => (\n            <article key={blog.id} className=\"group\">\n              <Link href={`/blogs/${blog.slug}`} className=\"block\">\n                <div className=\"grid md:grid-cols-4 gap-6\">\n                  {blog.coverImage && (\n                    <div className=\"md:col-span-1\">\n                      <div className=\"aspect-w-16 aspect-h-9 relative overflow-hidden rounded-lg\">\n                        <Image\n                          src={blog.coverImage}\n                          alt={blog.title}\n                          fill\n                          className=\"object-cover transition-transform duration-300 group-hover:scale-105\"\n                        />\n                      </div>\n                    </div>\n                  )}\n                  \n                  <div className={blog.coverImage ? \"md:col-span-3\" : \"md:col-span-4\"}>\n                    <div className=\"flex items-center text-sm text-gray-400 mb-2\">\n                      <time dateTime={blog.date}>{new Date(blog.date).toLocaleDateString('en-US', {\n                        year: 'numeric',\n                        month: 'long',\n                        day: 'numeric'\n                      })}</time>\n                      \n                      {blog.isProjectBlog && (\n                        <>\n                          <span className=\"mx-2\">•</span>\n                          <span className=\"text-blue-400\">Project Blog</span>\n                        </>\n                      )}\n                    </div>\n                    \n                    <h2 className=\"text-2xl font-semibold mb-2 group-hover:text-blue-400 transition-colors\">\n                      {blog.title}\n                    </h2>\n                    \n                    <p className=\"text-gray-300 mb-4\">{blog.summary}</p>\n                    \n                    <div className=\"flex flex-wrap gap-2\">\n                      {blog.tags.map((tag) => (\n                        <span key={tag} className=\"px-2 py-1 text-xs bg-gray-800 rounded-full text-gray-300\">\n                          {tag}\n                        </span>\n                      ))}\n                    </div>\n                  </div>\n                </div>\n              </Link>\n            </article>\n          ))}\n        </div>\n      </div>\n    </main>\n  );\n} "],"names":[],"mappings":";;;;;AAAA;AACA;AACA;;;;;AAEO,MAAM,WAAW;IACtB,OAAO;IACP,aAAa;AACf;AAEe,eAAe;IAC5B,MAAM,QAAQ,MAAM,CAAA,GAAA,oHAAA,CAAA,cAAW,AAAD;IAE9B,qBACE,8OAAC;QAAK,WAAU;kBACd,cAAA,8OAAC;YAAI,WAAU;;8BACb,8OAAC;oBAAG,WAAU;8BAAyC;;;;;;8BAEvD,8OAAC;oBAAI,WAAU;8BACZ,MAAM,GAAG,CAAC,CAAC,qBACV,8OAAC;4BAAsB,WAAU;sCAC/B,cAAA,8OAAC,4JAAA,CAAA,UAAI;gCAAC,MAAM,CAAC,OAAO,EAAE,KAAK,IAAI,EAAE;gCAAE,WAAU;0CAC3C,cAAA,8OAAC;oCAAI,WAAU;;wCACZ,KAAK,UAAU,kBACd,8OAAC;4CAAI,WAAU;sDACb,cAAA,8OAAC;gDAAI,WAAU;0DACb,cAAA,8OAAC,6HAAA,CAAA,UAAK;oDACJ,KAAK,KAAK,UAAU;oDACpB,KAAK,KAAK,KAAK;oDACf,IAAI;oDACJ,WAAU;;;;;;;;;;;;;;;;sDAMlB,8OAAC;4CAAI,WAAW,KAAK,UAAU,GAAG,kBAAkB;;8DAClD,8OAAC;oDAAI,WAAU;;sEACb,8OAAC;4DAAK,UAAU,KAAK,IAAI;sEAAG,IAAI,KAAK,KAAK,IAAI,EAAE,kBAAkB,CAAC,SAAS;gEAC1E,MAAM;gEACN,OAAO;gEACP,KAAK;4DACP;;;;;;wDAEC,KAAK,aAAa,kBACjB;;8EACE,8OAAC;oEAAK,WAAU;8EAAO;;;;;;8EACvB,8OAAC;oEAAK,WAAU;8EAAgB;;;;;;;;;;;;;;8DAKtC,8OAAC;oDAAG,WAAU;8DACX,KAAK,KAAK;;;;;;8DAGb,8OAAC;oDAAE,WAAU;8DAAsB,KAAK,OAAO;;;;;;8DAE/C,8OAAC;oDAAI,WAAU;8DACZ,KAAK,IAAI,CAAC,GAAG,CAAC,CAAC,oBACd,8OAAC;4DAAe,WAAU;sEACvB;2DADQ;;;;;;;;;;;;;;;;;;;;;;;;;;;2BAxCT,KAAK,EAAE;;;;;;;;;;;;;;;;;;;;;AAsDjC"}},
    {"offset": {"line": 475, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 486, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":""}},
    {"offset": {"line": 486, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}}]
}